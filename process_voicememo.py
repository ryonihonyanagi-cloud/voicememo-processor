#!/usr/bin/env python3
"""
Voice Memo Processor v2.1
  Phase 1: USB mount ‚Üí WAV to MP3 ‚Üí Google Drive (date folders)
  Phase 2: Google Drive MP3 ‚Üí mlx-whisper local ‚Üí Gemini summary ‚Üí Markdown

Changes from v2:
- Anti-hallucination: condition_on_previous_text=False, hallucination_silence_threshold
- Post-processing filter for repetitive/hallucinated segments
- macOS push notifications for progress tracking
"""

import json
import logging
import os
import re
import shutil
import subprocess
import tempfile
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path

import dotenv
import mlx_whisper
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Configuration
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

VOICEMEMO_MOUNT = Path(os.environ.get(
    "VOICEMEMO_MOUNT", 
    "/Volumes/VOICEMEMO/RECORD"
))
FFMPEG_PATH = shutil.which("ffmpeg") or "/opt/homebrew/bin/ffmpeg"
FFPROBE_PATH = shutil.which("ffprobe") or "/opt/homebrew/bin/ffprobe"

MARKDOWN_OUTPUT_DIR = Path(os.environ.get(
    "MARKDOWN_OUTPUT_DIR",
    str(Path.home() / "Documents/GitHub/llm-knowledge-base/0-inbox/voicememo")
))
MP3_BASE_DIR = Path(os.environ.get(
    "MP3_BASE_DIR",
    str(Path.home() / "Library/CloudStorage/GoogleDrive-ryo.nihonyanagi@10xc.jp/„Éû„Ç§„Éâ„É©„Ç§„Éñ/Voicememo")
))

SCRIPT_DIR = Path(__file__).parent.resolve()
MANIFEST_PATH = SCRIPT_DIR / "processed_files.json"
STATUS_PATH = SCRIPT_DIR / "status.json"
USER_PROFILE_PATH = SCRIPT_DIR / "user_profile.json"  # Accumulating persona context
LOG_DIR = SCRIPT_DIR / "logs"
STAGING_DIR = SCRIPT_DIR / "staging"  # Local MP3 copies to avoid FUSE deadlock

MP3_BITRATE = "64k"
WHISPER_MODEL_REPO = "mlx-community/whisper-large-v3-turbo"
GEMINI_MODEL = "gemini-1.5-pro"

# File naming: 2026-02-11-17-53-58.WAV
FILENAME_PATTERN = re.compile(
    r"(\d{4})-(\d{2})-(\d{2})-(\d{2})-(\d{2})-(\d{2})\.(WAV|mp3)", re.IGNORECASE
)

logger = logging.getLogger("voicememo")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# User Profile (Context Accumulation for SNS Posts)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def load_user_profile() -> dict:
    """Load or initialize the user profile for context-aware SNS post generation."""
    if USER_PROFILE_PATH.exists():
        try:
            return json.loads(USER_PROFILE_PATH.read_text(encoding="utf-8"))
        except Exception:
            pass
    return {
        "frequent_topics": [],          # Topics that come up often (accumulated)
        "tone_description": "",          # Writing tone/style inferred from posts
        "example_posts": [],             # Last N successful/generated posts (for style reference)
        "interests": [],                 # Inferred interest areas
        "last_updated": ""
    }


def save_user_profile(profile: dict):
    """Persist the updated user profile."""
    import datetime
    profile["last_updated"] = datetime.datetime.now().isoformat()
    USER_PROFILE_PATH.write_text(json.dumps(profile, ensure_ascii=False, indent=2), encoding="utf-8")


def update_user_profile(date: str, summary_data: dict, profile: dict) -> dict:
    """Ask Gemini to merge today's insights into the running user profile."""
    new_posts = summary_data.get("x_threads_posts", [])
    new_topics = summary_data.get("deep_conversations", [])
    today_summary = summary_data.get("summary", "")

    # Keep a rolling window of the 20 most recent posts as style examples
    all_posts = profile.get("example_posts", []) + [
        p.get("content", "") for p in new_posts if p.get("content")
    ]
    profile["example_posts"] = all_posts[-20:]

    # Ask Gemini to update the topic list and tone description
    topics_block = "\n".join(
        f"- {dc.get('topic', '')}: {dc.get('insight', '')}" for dc in new_topics
    )
    examples_block = "\n".join(f"- {p}" for p in all_posts[-5:])

    update_prompt = f"""„ÅÇ„Å™„Åü„ÅØSNSÊäïÁ®ø„ÅÆ„Éë„Éº„ÇΩ„Éä„É©„Ç§„Ç∫„ÇíÊãÖÂΩì„Åô„ÇãAI„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆÊÉÖÂ†±„Çí„ÇÇ„Å®„Å´„ÄÅ„Åì„ÅÆ„É¶„Éº„Ç∂„Éº„ÅÆÁô∫‰ø°„Çπ„Çø„Ç§„É´„ÇÑ„Çà„ÅèË™û„Çã„ÉÜ„Éº„Éû„ÅÆ„Éó„É≠„Éï„Ç£„Éº„É´„ÇíÊõ¥Êñ∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê‰ªäÊó•„ÅÆÊó•‰ªò„Äë{date}
„Äê‰ªäÊó•„ÅÆ„Çµ„Éû„É™„Éº„Äë{today_summary}

„Äê‰ªäÊó•„ÅÆÊ∑±„ÅÑ‰ºöË©±„ÉªÊ∞ó„Å•„Åç„Äë
{topics_block}

„ÄêÈÅéÂéª„ÅÆÊäïÁ®ø‰æãÔºàÊúÄËøë„ÅÆ„ÇÇ„ÅÆÔºâ„Äë
{examples_block}

„ÄêÁèæÂú®„ÅÆ„Éó„É≠„Éï„Ç£„Éº„É´„Äë
„Çà„ÅèË™û„Çã„ÉÜ„Éº„Éû: {', '.join(profile.get('frequent_topics', []))}
Êñá‰Ωì„Éª„Éà„Éº„É≥: {profile.get('tone_description', '(Êú™Ë®≠ÂÆö)')}
ËààÂë≥„ÉªÈñ¢ÂøÉ: {', '.join(profile.get('interests', []))}

‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„ÅßÊõ¥Êñ∞„Åï„Çå„Åü„Éó„É≠„Éï„Ç£„Éº„É´„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
{{
  "frequent_topics": ["„ÉÜ„Éº„Éû1", "„ÉÜ„Éº„Éû2", ...],  // ‰ªäÊó•„ÅÆÂÜÖÂÆπ„ÇÇË∏è„Åæ„Åà„ÄÅÈáçË¶ÅÂ∫¶„ÅåÈ´ò„ÅÑÈ†Ü„Å´ÊúÄÂ§ß15‰ª∂
  "tone_description": "„Åì„ÅÆ„É¶„Éº„Ç∂„Éº„ÅÆÊñá‰Ωì„ÉªÁô∫‰ø°„Çπ„Çø„Ç§„É´„ÅÆË™¨ÊòéÔºà3„Äú5ÊñáÔºâ",
  "interests": ["Èñ¢ÂøÉÈ†òÂüü1", "Èñ¢ÂøÉÈ†òÂüü2", ...]   // ‰∏ª„Å™Èñ¢ÂøÉÈ†òÂüü„ÄÅÊúÄÂ§ß10‰ª∂
}}

„É´„Éº„É´:
- frequent_topics„ÅØ‰ªäÊó•Êñ∞„Åü„Å´ÁôªÂ†¥„Åó„Åü„ÉÜ„Éº„Éû„ÇÇËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- Êó¢Â≠ò„ÅÆ„ÉÜ„Éº„Éû„Å®ÈáçË§á„Åô„Çã„ÇÇ„ÅÆ„ÅØ„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ
- tone_description„ÅØÈÅéÂéª„ÅÆÊäïÁ®ø‰æã„Åã„ÇâÊñá‰Ωì„Éª„Éà„Éº„É≥„ÉªË®ÄËëâÈÅ∏„Å≥„ÅÆÂÇæÂêë„ÇíÊèèÂÜô„Åó„Å¶„Åè„Å†„Åï„ÅÑ
- ÂÖ®„Å¶Êó•Êú¨Ë™û"""

    try:
        result = _call_summary_api(update_prompt)
        if isinstance(result, dict):
            if result.get("frequent_topics"):
                profile["frequent_topics"] = result["frequent_topics"]
            if result.get("tone_description"):
                profile["tone_description"] = result["tone_description"]
            if result.get("interests"):
                profile["interests"] = result["interests"]
    except Exception as e:
        logger.warning(f"  Profile update failed (non-critical): {e}")

    return profile


def _build_profile_context(profile: dict) -> str:
    """Format the user profile as a context block for injection into prompts."""
    if not profile.get("frequent_topics") and not profile.get("tone_description"):
        return ""  # No profile yet ‚Äî first run
    parts = []
    if profile.get("frequent_topics"):
        parts.append(f"„Çà„ÅèË™û„Çã„ÉÜ„Éº„Éû: {', '.join(profile['frequent_topics'][:10])}")
    if profile.get("interests"):
        parts.append(f"Èñ¢ÂøÉÈ†òÂüü: {', '.join(profile['interests'][:6])}")
    if profile.get("tone_description"):
        parts.append(f"Êñá‰Ωì„Éª„Éà„Éº„É≥: {profile['tone_description']}")
    if profile.get("example_posts"):
        examples = profile["example_posts"][-3:]
        parts.append("ÈÅéÂéª„ÅÆÊäïÁ®ø‰æã:")
        for ex in examples:
            parts.append(f"  - {ex[:120]}{'...' if len(ex) > 120 else ''}")
    return "\n".join(parts)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# macOS Notifications
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def notify(title: str, message: str, sound: str = ""):
    """Send a macOS notification via osascript."""
    try:
        sound_part = f' sound name "{sound}"' if sound else ""
        script = (
            f'display notification "{message}" '
            f'with title "{title}"{sound_part}'
        )
        subprocess.Popen(
            ["osascript", "-e", script],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
    except Exception:
        pass  # Notifications are best-effort, never block processing


def update_status(
    status: str = "idle",
    phase: int = 0,
    phase_label: str = "",
    current_file: str = "",
    files_total: int = 0,
    files_completed: int = 0,
    last_error: str | None = None,
):
    """Write current processing status to status.json for the menu bar monitor."""
    try:
        data = {
            "status": status,
            "phase": phase,
            "phase_label": phase_label,
            "current_file": current_file,
            "files_total": files_total,
            "files_completed": files_completed,
            "last_error": last_error,
            "last_updated": datetime.now().isoformat(),
        }
        tmp = STATUS_PATH.with_suffix(".tmp")
        tmp.write_text(json.dumps(data, ensure_ascii=False), encoding="utf-8")
        tmp.replace(STATUS_PATH)
    except Exception:
        pass  # Status updates are best-effort


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Logging
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def setup_logging():
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    log_file = LOG_DIR / f"voicememo-{datetime.now().strftime('%Y%m%d')}.log"

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(),
        ],
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Manifest (processed files tracking)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def load_manifest() -> dict:
    if MANIFEST_PATH.exists():
        return json.loads(MANIFEST_PATH.read_text(encoding="utf-8"))
    return {"version": 2, "copied": {}, "transcribed": {}}


def save_manifest(manifest: dict):
    MANIFEST_PATH.write_text(
        json.dumps(manifest, indent=2, ensure_ascii=False), encoding="utf-8"
    )


def migrate_manifest(manifest: dict) -> dict:
    """Migrate v1 manifest to v2 format if needed."""
    if manifest.get("version") == 2:
        return manifest

    new_manifest = {"version": 2, "copied": {}, "transcribed": {}}

    # Migrate v1 "processed" entries
    for filename, entry in manifest.get("processed", {}).items():
        mp3_name = filename.replace(".WAV", ".mp3").replace(".wav", ".mp3")

        # Mark as copied
        new_manifest["copied"][filename] = {
            "size_bytes": entry.get("size_bytes", 0),
            "copied_at": entry.get("processed_at", ""),
            "mp3_name": mp3_name,
            "date": entry.get("date", ""),
            "time": entry.get("time", ""),
            "time_full": entry.get("time_full", ""),
        }

        # If it was fully transcribed, mark that too
        if entry.get("status") == "completed" and "transcript_text" in entry:
            new_manifest["transcribed"][mp3_name] = {
                "transcribed_at": entry.get("processed_at", ""),
                "duration_seconds": entry.get("duration_seconds", 0),
                "date": entry.get("date", ""),
                "time": entry.get("time", ""),
                "time_full": entry.get("time_full", ""),
                "transcript_text": entry.get("transcript_text", ""),
                "segments": entry.get("segments", []),
            }

    return new_manifest


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 1: USB ‚Üí MP3 ‚Üí Google Drive
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def discover_wav_files() -> list[dict]:
    """Scan USB device for WAV files."""
    files = []
    for wav_path in sorted(VOICEMEMO_MOUNT.glob("*.WAV")):
        match = FILENAME_PATTERN.match(wav_path.name)
        if match:
            y, m, d, hh, mm, ss, _ext = match.groups()
            files.append(
                {
                    "path": wav_path,
                    "filename": wav_path.name,
                    "date": f"{y}-{m}-{d}",
                    "time": f"{hh}:{mm}",
                    "time_full": f"{hh}:{mm}:{ss}",
                    "size": wav_path.stat().st_size,
                }
            )
    return files


def convert_wav_to_mp3(wav_path: Path, mp3_path: Path) -> Path:
    """Convert WAV to MP3 using ffmpeg."""
    mp3_path.parent.mkdir(parents=True, exist_ok=True)
    cmd = [
        FFMPEG_PATH,
        "-y",
        "-i",
        str(wav_path),
        "-vn",
        "-ac", "1",
        "-ar", "16000",
        "-acodec", "libmp3lame",
        "-b:a", MP3_BITRATE,
        str(mp3_path),
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
    if result.returncode != 0:
        raise RuntimeError(f"ffmpeg conversion failed: {result.stderr[-500:]}")
    return mp3_path


def _copy_to_google_drive(src_path: Path, dest_path: Path):
    """Copy a file to Google Drive using raw byte write (avoids fcopyfile deadlock).

    Write-only operation ‚Äî Google Drive FUSE handles writes fine,
    the deadlock only occurs on reads while Drive is syncing.
    """
    dest_path.parent.mkdir(parents=True, exist_ok=True)
    with open(src_path, "rb") as src, open(dest_path, "wb") as dst:
        while True:
            chunk = src.read(1024 * 1024)
            if not chunk:
                break
            dst.write(chunk)


def phase1_copy_from_usb(manifest: dict) -> set[str]:
    """
    Phase 1: Convert WAV files from USB to MP3.
    Saves to local staging dir first (for reliable Phase 2 reads),
    then copies to Google Drive (for backup/sync).
    Returns set of dates that had new files copied.
    """
    if not VOICEMEMO_MOUNT.exists():
        logger.info("VOICEMEMO not mounted, skipping Phase 1")
        return set()

    wav_files = discover_wav_files()
    new_files = [
        f for f in wav_files
        if f["filename"] not in manifest["copied"]
    ]

    if not new_files:
        logger.info("Phase 1: No new WAV files on device")
        return set()

    logger.info(f"Phase 1: {len(new_files)} new WAV file(s) to convert")
    notify("Voice Memo", f"Phase 1: {len(new_files)}‰ª∂„ÅÆWAV„ÇíÂ§âÊèõ‰∏≠...")
    update_status("processing", 1, "MP3Â§âÊèõ‰∏≠", files_total=len(new_files))
    new_dates = set()

    STAGING_DIR.mkdir(parents=True, exist_ok=True)

    for i, file_info in enumerate(new_files, 1):
        filename = file_info["filename"]
        date = file_info["date"]
        mp3_name = filename.replace(".WAV", ".mp3").replace(".wav", ".mp3")

        # Convert to local staging first (fast, reliable local disk)
        staging_path = STAGING_DIR / mp3_name

        # Google Drive: organized by date folder
        gdrive_path = MP3_BASE_DIR / date / mp3_name

        logger.info(f"  Converting: {filename}")
        notify("Voice Memo", f"MP3Â§âÊèõ‰∏≠ ({i}/{len(new_files)}): {filename}")
        update_status("processing", 1, "MP3Â§âÊèõ‰∏≠", filename, len(new_files), i - 1)

        try:
            # Step 1: Convert WAV ‚Üí MP3 to local staging
            convert_wav_to_mp3(file_info["path"], staging_path)
            mp3_size_mb = staging_path.stat().st_size / (1024 * 1024)
            logger.info(f"  ‚Üí {staging_path.name} ({mp3_size_mb:.1f} MB) [staging]")

            # Step 2: Copy to Google Drive (write-only, non-blocking)
            try:
                _copy_to_google_drive(staging_path, gdrive_path)
                logger.info(f"  ‚Üí Copied to Google Drive: {gdrive_path.parent.name}/{gdrive_path.name}")
            except OSError as e:
                # Google Drive write failure is non-fatal ‚Äî staging copy is enough
                logger.warning(f"  Google Drive copy failed (will retry later): {e}")

            manifest["copied"][filename] = {
                "size_bytes": file_info["size"],
                "copied_at": datetime.now().isoformat(),
                "mp3_name": mp3_name,
                "mp3_path": str(gdrive_path),
                "staging_path": str(staging_path),
                "date": date,
                "time": file_info["time"],
                "time_full": file_info["time_full"],
            }
            save_manifest(manifest)
            new_dates.add(date)

        except Exception as e:
            logger.error(f"  FAILED converting {filename}: {e}")
            update_status("processing", 1, "MP3Â§âÊèõ‰∏≠", filename, len(new_files), i - 1, last_error=str(e)[:100])
            continue

    if new_dates:
        update_status("processing", 1, "MP3Â§âÊèõÂÆå‰∫Ü", files_total=len(new_files), files_completed=len(new_files))
        notify(
            "Voice Memo",
            f"Phase 1ÂÆå‰∫Ü: {len(new_files)}‰ª∂„ÇíÂ§âÊèõÊ∏à„Åø„ÄÇUSB„ÅØÂÆâÂÖ®„Å´Âèñ„ÇäÂ§ñ„Åõ„Åæ„Åô",
            sound="Glass",
        )

    return new_dates


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 2: Transcribe from Google Drive MP3
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def get_audio_duration(file_path: Path) -> float:
    """Get duration in seconds using ffprobe."""
    cmd = [
        FFPROBE_PATH,
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "csv=p=0",
        str(file_path),
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
    if result.returncode != 0:
        raise RuntimeError(f"ffprobe failed: {result.stderr[-500:]}")
    return float(result.stdout.strip())


# Known Whisper hallucination phrases (from YouTube training data leakage etc.)
HALLUCINATION_PHRASES = {
    "„ÅîË¶ñËÅ¥„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„Åü",
    "„ÉÅ„É£„É≥„Éç„É´ÁôªÈå≤„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô",
    "„ÉÅ„É£„É≥„Éç„É´ÁôªÈå≤„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô",
    "„ÅÑ„ÅÑ„Å≠„Éú„Çø„É≥„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
    "„Ç∞„ÉÉ„Éâ„Éú„Çø„É≥„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô",
    "„ÅîÊ∏ÖËÅ¥„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„Åü",
    "Â≠óÂπï„ÅØËá™ÂãïÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô",
    "„Åì„ÅÆÂãïÁîª„ÅåÊ∞ó„Å´ÂÖ•„Å£„Åü„Çâ",
    "Ê¨°Âõû„ÇÇ„ÅäÊ•Ω„Åó„Åø„Å´",
    "Thanks for watching",
    "Please subscribe",
    "Like and subscribe",
}

# Common short hallucination tokens (nonsense fragments Whisper generates)
HALLUCINATION_TOKENS = {"arte", "artearte", "arteartearte"}


def is_hallucination(text: str) -> bool:
    """Detect hallucinated/repetitive segments from Whisper."""
    t = text.strip()
    if not t:
        return True

    # Too short to be meaningful (single char or just punctuation/filler)
    if len(t) <= 2:
        return True

    # Known hallucination phrases (YouTube training data leakage)
    if t in HALLUCINATION_PHRASES:
        return True

    # Short nonsense tokens Whisper hallucinates (e.g. "arte", "artearte")
    # Catch standalone, embedded ("„Å£artearte"), or mixed ("‰ªäÂõûartearte")
    if re.search(r"(arte){1,}", t):
        return True

    # Whisper hallucination: "oud" / "oudoud..." repetitions
    if re.search(r"(oud){1,}", t) and len(re.sub(r"(oud)+", "", t).strip()) < 5:
        return True

    # Whisper hallucination: "amb" repeated ("amb amb amb...")
    if re.search(r"(amb[\s]*){2,}", t):
        return True

    # Whisper hallucination: "Honor" repeated
    if re.search(r"(Honor[\s]*){2,}", t):
        return True

    # Whisper hallucination: "SCO" repeated
    if re.search(r"(SCO[\s]*){2,}", t):
        return True

    # Detect repeated characters like "„ÅÇ„ÅÇ„ÅÇ„ÅÇ", "„ÅÜ„ÅÜ„ÅÜ„ÅÜ", "„Åà„Åà„Åà"
    if re.match(r"^(.)\1{2,}$", t):
        return True

    # Detect patterns like "„ÅÇ„ÅÇ„ÅÇ„ÅÇ„ÅÇ„ÅÇ „ÅØ„Åü" (repeated chars + short word)
    if re.match(r"^(.)\1{3,}(\s+.{0,4})?$", t):
        return True

    # Detect short phrase repetition (2-6 chars repeated 3+ times)
    # Catches: "Ë≥™Âïè„ÅØË≥™Âïè„ÅØË≥™Âïè„ÅØ...", "„ÅäÂ∫ó„ÅÆ„ÅäÂ∫ó„ÅÆ„ÅäÂ∫ó„ÅÆ...", "Êõ∏„Åë„Å¶Êõ∏„Åë„Å¶Êõ∏„Åë„Å¶..."
    if re.search(r"(.{2,6})\1{2,}", t):
        # Only flag if the repeated part dominates the text (>50%)
        m = re.search(r"(.{2,6})\1{2,}", t)
        if m and len(m.group(0)) > len(t) * 0.5:
            return True

    # Detect long strings with >60% same character (e.g. "„É®„Éº„Éä„É®„É®„É®„É®„É®„É®„É®...")
    if len(t) > 10:
        from collections import Counter
        char_counts = Counter(t.replace(" ", ""))
        if char_counts and char_counts.most_common(1)[0][1] / len(t.replace(" ", "")) > 0.6:
            return True

    return False


def _normalize_text(text: str) -> str:
    """Normalize text for comparison (strip punctuation variants)."""
    # Remove trailing punctuation differences: "„Åä„ÇÑ„Åô„Åø„Å™„Åï„ÅÑ" vs "„Åä„ÇÑ„Åô„Åø„Å™„Åï„ÅÑ„ÄÇ"
    return re.sub(r"[„ÄÇ„ÄÅÔºÅÔºü!?.,\s]+$", "", text.strip())


def filter_hallucinated_segments(segments: list[dict]) -> list[dict]:
    """Remove hallucinated/repetitive segments from transcription output."""
    if not segments:
        return segments

    filtered = []
    # Track recent texts (sliding window) to catch non-consecutive repetition
    recent_texts: list[str] = []  # normalized texts of last N segments
    WINDOW_SIZE = 10
    MAX_REPEATS_IN_WINDOW = 2  # Allow max 2 same texts in a window of 10

    for seg in segments:
        text = seg["text"].strip()

        # Skip individually hallucinated segments
        if is_hallucination(text):
            continue

        # Check repetition within sliding window
        norm = _normalize_text(text)
        occurrences = recent_texts.count(norm)
        if occurrences >= MAX_REPEATS_IN_WINDOW:
            continue  # Too many repeats in recent window, skip

        recent_texts.append(norm)
        if len(recent_texts) > WINDOW_SIZE:
            recent_texts.pop(0)

        filtered.append(seg)

    before = len(segments)
    after = len(filtered)
    if before != after:
        logger.info(
            f"  Hallucination filter: {before} ‚Üí {after} segments "
            f"({before - after} removed)"
        )

    return filtered


def _run_whisper(audio_path: str) -> dict:
    """Run mlx-whisper transcription on an audio file path."""
    return mlx_whisper.transcribe(
        audio_path,
        path_or_hf_repo=WHISPER_MODEL_REPO,
        language="ja",
        word_timestamps=True,
        condition_on_previous_text=False,   # Prevent hallucination cascading
        compression_ratio_threshold=2.4,     # Reject overly repetitive output
        no_speech_threshold=0.6,             # Detect non-speech segments
        # NOTE: hallucination_silence_threshold intentionally omitted (22x slower)
        # Post-processing filter handles hallucination cleanup instead
    )


def _find_local_mp3(mp3_path: Path) -> Path | None:
    """Check if a local staging copy exists for this MP3."""
    staging_path = STAGING_DIR / mp3_path.name
    if staging_path.exists() and staging_path.stat().st_size > 1024:
        return staging_path
    return None


def transcribe_local(mp3_path: Path) -> dict:
    """Transcribe audio using mlx-whisper locally (Apple Silicon GPU).

    Prefers local staging copy over Google Drive to avoid FUSE deadlock.
    Falls back to copying from Google Drive with retries if no staging copy.
    """
    logger.info(f"  Transcribing locally: {mp3_path.name}")
    start_time = time.time()

    # Check if file is on Google Drive (FUSE mount) ‚Äî may need local copy
    is_cloud = "CloudStorage" in str(mp3_path) or "GoogleDrive" in str(mp3_path)

    if is_cloud:
        # Prefer local staging copy (written in Phase 1, no FUSE issues)
        local_path = _find_local_mp3(mp3_path)
        if local_path:
            logger.info(f"  Using staging copy: {local_path}")
            result = _run_whisper(str(local_path))
        else:
            # No staging copy ‚Äî fall back to copying from Google Drive with retries
            tmp_dir = Path(tempfile.mkdtemp(prefix="voicememo_"))
            tmp_path = tmp_dir / mp3_path.name
            try:
                max_cp_retries = 5
                for cp_attempt in range(1, max_cp_retries + 1):
                    logger.info(f"  Copying from Google Drive to local temp (attempt {cp_attempt}/{max_cp_retries})...")
                    try:
                        with open(mp3_path, "rb") as src, open(tmp_path, "wb") as dst:
                            while True:
                                chunk = src.read(1024 * 1024)
                                if not chunk:
                                    break
                                dst.write(chunk)
                        break  # Success
                    except OSError as e:
                        logger.warning(f"  Copy failed: {e}")
                        tmp_path.unlink(missing_ok=True)
                        if cp_attempt < max_cp_retries:
                            delay = 30 * cp_attempt  # 30s, 60s, 90s, 120s
                            logger.info(f"  Waiting {delay}s before retry...")
                            time.sleep(delay)
                        else:
                            raise RuntimeError(
                                f"Copy failed after {max_cp_retries} attempts: {e}"
                            )
                logger.info(f"  Transcribing from: {tmp_path}")
                result = _run_whisper(str(tmp_path))
            finally:
                tmp_path.unlink(missing_ok=True)
                tmp_dir.rmdir()
                logger.info(f"  Cleaned up temp copy")
    else:
        result = _run_whisper(str(mp3_path))

    elapsed = time.time() - start_time
    duration = result.get("segments", [{}])[-1].get("end", 0) if result.get("segments") else 0

    # If segments didn't give us duration, use ffprobe
    if duration == 0:
        try:
            duration = get_audio_duration(mp3_path)
        except Exception:
            pass

    segments = [
        {
            "start": seg["start"],
            "end": seg["end"],
            "text": seg["text"],
        }
        for seg in result.get("segments", [])
        if seg.get("text", "").strip()
    ]

    # Post-processing: filter out hallucinated/repetitive segments
    segments = filter_hallucinated_segments(segments)

    # Rebuild clean text from filtered segments
    text = " ".join(seg["text"].strip() for seg in segments)

    logger.info(
        f"  Transcribed: {len(segments)} segments, "
        f"{duration:.0f}s audio in {elapsed:.1f}s"
    )

    return {
        "text": text,
        "segments": segments,
        "duration": duration,
    }


def discover_untranscribed_mp3s(manifest: dict) -> list[dict]:
    """Find MP3 files in Google Drive that haven't been transcribed yet."""
    untranscribed = []

    for wav_name, copy_entry in manifest["copied"].items():
        mp3_name = copy_entry["mp3_name"]

        # Already transcribed?
        if mp3_name in manifest["transcribed"]:
            continue

        # Find the MP3 file
        date = copy_entry["date"]
        mp3_path = MP3_BASE_DIR / date / mp3_name

        # Also check flat directory (legacy from v1)
        if not mp3_path.exists():
            mp3_path = MP3_BASE_DIR / mp3_name

        if not mp3_path.exists():
            logger.warning(f"  MP3 not found: {mp3_name} (skipping)")
            continue

        untranscribed.append(
            {
                "mp3_path": mp3_path,
                "mp3_name": mp3_name,
                "date": copy_entry["date"],
                "time": copy_entry["time"],
                "time_full": copy_entry.get("time_full", copy_entry["time"] + ":00"),
            }
        )

    return untranscribed


def phase2_transcribe(manifest: dict) -> set[str]:
    """
    Phase 2: Transcribe untranscribed MP3 files from Google Drive using mlx-whisper.
    Returns set of dates that had new transcriptions.
    """
    untranscribed = discover_untranscribed_mp3s(manifest)

    if not untranscribed:
        logger.info("Phase 2: No untranscribed MP3 files")
        return set()

    logger.info(f"Phase 2: {len(untranscribed)} file(s) to transcribe")
    notify("Voice Memo", f"Phase 2: {len(untranscribed)}‰ª∂„ÅÆÊñáÂ≠óËµ∑„Åì„ÅóÈñãÂßã...")
    update_status("processing", 2, "ÊñáÂ≠óËµ∑„Åì„Åó‰∏≠", files_total=len(untranscribed))
    new_dates = set()
    success_count = 0
    fail_count = 0

    # Count expected files per date for partial-failure detection
    date_file_counts: dict[str, int] = {}
    for mp3_info in untranscribed:
        d = mp3_info["date"]
        date_file_counts[d] = date_file_counts.get(d, 0) + 1

    for i, mp3_info in enumerate(untranscribed, 1):
        mp3_name = mp3_info["mp3_name"]
        logger.info(f"Processing: {mp3_name}")
        notify("Voice Memo", f"ÊñáÂ≠óËµ∑„Åì„Åó‰∏≠ ({i}/{len(untranscribed)}): {mp3_info['time']}")
        update_status("processing", 2, "ÊñáÂ≠óËµ∑„Åì„Åó‰∏≠", mp3_name, len(untranscribed), i - 1)

        try:
            transcript = transcribe_local(mp3_info["mp3_path"])

            manifest["transcribed"][mp3_name] = {
                "transcribed_at": datetime.now().isoformat(),
                "duration_seconds": transcript["duration"],
                "date": mp3_info["date"],
                "time": mp3_info["time"],
                "time_full": mp3_info["time_full"],
                "transcript_text": transcript["text"],
                "segments": transcript["segments"],
            }
            save_manifest(manifest)
            new_dates.add(mp3_info["date"])
            success_count += 1

            # Clean up staging copy after successful transcription
            staging_path = STAGING_DIR / mp3_name
            if staging_path.exists():
                staging_path.unlink()
                logger.info(f"  Cleaned up staging file: {mp3_name}")

        except Exception as e:
            logger.error(f"  FAILED transcribing {mp3_name}: {e}", exc_info=True)
            update_status("processing", 2, "ÊñáÂ≠óËµ∑„Åì„Åó‰∏≠", mp3_name, len(untranscribed), i - 1, last_error=str(e)[:100])
            fail_count += 1
            continue

    if fail_count > 0:
        logger.warning(f"Phase 2 completed with {fail_count} failures out of {len(untranscribed)} files")
        notify("Voice Memo", f"‚ö† ÊñáÂ≠óËµ∑„Åì„Åó: {success_count}ÊàêÂäü / {fail_count}Â§±ÊïóÔºàÊ¨°ÂõûËá™Âãï„É™„Éà„É©„Ç§Ôºâ")

    return new_dates


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 3: Gemini summary + Markdown
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def retry_with_backoff(func, max_retries=3, base_delay=5):
    for attempt in range(max_retries):
        try:
            return func()
        except google_exceptions.RetryError as e:
            if attempt == max_retries - 1:
                raise
            delay = base_delay * (2**attempt)
            logger.warning(
                f"API error ({type(e).__name__}), retrying in {delay}s "
                f"(attempt {attempt + 1}/{max_retries})"
            )
            time.sleep(delay)
        except (google_exceptions.GoogleAPIError, Exception) as e:
            if attempt == max_retries - 1:
                raise
            delay = base_delay * (2**attempt)
            logger.warning(f"API error ({e}), retrying in {delay}s")
            time.sleep(delay)
    raise RuntimeError(f"Failed after {max_retries} retries")


def _build_transcript_block(recordings: list[dict]) -> str:
    """Build a transcript text block from a list of recordings."""
    block = ""
    for rec in recordings:
        block += (
            f"\n--- Recording at {rec['time']} "
            f"({rec['duration_min']:.0f} min) ---\n"
        )
        clean_segs = filter_hallucinated_segments(rec["segments"])
        clean_text = " ".join(s["text"].strip() for s in clean_segs if s["text"].strip())
        block += (clean_text or rec["transcript_text"]) + "\n"
    return block


def _call_summary_api(prompt: str) -> dict:
    """Call Gemini API with a summary prompt and return parsed JSON."""
    model = genai.GenerativeModel(
        model_name=GEMINI_MODEL,
        system_instruction=(
            "You are a helpful assistant that summarizes voice memos "
            "in Japanese. Always respond with valid JSON. Do not return Markdown code blocks like ```json, just raw JSON."
        ),
    )
    response = model.generate_content(
        prompt,
        generation_config=genai.GenerationConfig(
            response_mime_type="application/json",
            temperature=0.5,
            max_output_tokens=8000,
        )
    )
    return json.loads(response.text)


MAX_CHARS_PER_CHUNK = 20000  # ~7,000 tokens ‚Äî safe for 30k TPM limit


def summarize_transcripts(
    date: str, recordings: list[dict], profile: dict | None = None
) -> dict:
    """Call GPT-4o to generate summary and highlights.

    If the transcript is too long for a single API call, it is split into
    chunks, each chunk is summarized individually, and the partial summaries
    are merged in a final API call.
    """
    profile = profile or {}
    profile_ctx = _build_profile_context(profile)
    profile_section = f"""
„ÄêÊäïÁ®øËÄÖ„Éó„É≠„Éï„Ç£„Éº„É´Ôºà„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÔºâ„Äë
{profile_ctx}
""" if profile_ctx else ""

    # SNS post instructions (shared between single / merge prompts)
    _sns_instructions = f"""{profile_section}
‰ª•‰∏ã„ÅÆË¶≥ÁÇπ„Åß„ÄÅSNS„Å´ÊäïÁ®ø„Åß„Åç„Çã„Éù„Çπ„ÉàÊ°à„Çí5„Äú10‰ª∂ÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åù„Çå„Åû„ÇåÁï∞„Å™„Çã„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„ÉªËßíÂ∫¶„Éª„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêÂØæÂøú„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„Å®Âûã„ÅÆÊåáÂÆö„Äë
„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„ÅØ„ÄåX„Äç„ÄåX„Çπ„É¨„ÉÉ„Éâ„Äç„ÄåThreads„Äç„ÄåInstagram„Äç„Åã„ÇâÂàÜÊï£„Åï„Åõ„Å¶ÈÅ∏„Çì„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
- X: „ÄêÊ∞ó„Å•„ÅçÂûã„Äë„ÄêÂïè„ÅÑ„Åã„ÅëÂûã„Äë„ÄêÊÑèË¶ãÂûã„Äë„ÄêÂºïÁî®Âûã„Äë„Å™„Å©„ÄÇÂøÖ„ÅöÂÖ∑‰Ωì‰æã„ÇÑËÉåÊôØ„Çí„Åó„Å£„Åã„ÇäÊõ∏„ÅçËæº„Åø„ÄÅÈôêÁïå„ÅÆ140ÊñáÂ≠óÔºàÊúÄ‰Ωé„Åß„ÇÇ120ÊñáÂ≠óÔºâ„Å´Ëøë„Å•„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁü≠„Åô„Åé„ÇãÂçòÊñá„ÅØÁµ∂ÂØæ„Å´NG„Åß„Åô„ÄÇ
- X„Çπ„É¨„ÉÉ„Éâ: „ÄêX„Çπ„É¨„ÉÉ„ÉâÂûã„ÄëHook(1ÊäïÁõÆ) ‚Üí Êú¨Ë´ñ(Ë§áÊï∞Êäï) ‚Üí CTA(ÊúÄÁµÇÊäï)„ÄÇ1„ÉÑ„Ç§„Éº„Éà„Å´„Å§„ÅçÂøÖ„Åö100„Äú140ÊñáÂ≠óÊõ∏„Åç„ÄÅ„Åù„Çå„ÇíÊîπË°åÔºà\\nÔºâ„ÅßÂå∫Âàá„Å£„Å¶3„Äú7„ÉÑ„Ç§„Éº„ÉàÂàÜ„ÅÆÈï∑Á∑®„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- Threads: „ÄêThreadsÁî®„É≠„É≥„Ç∞„ÄëÊúÄ‰Ωé„Åß„ÇÇ300ÊñáÂ≠ó„ÄÅ„Åß„Åç„Çå„Å∞500ÊñáÂ≠óÁ®ãÂ∫¶„Åß„ÄÅÊÄùËÄÉ„Éó„É≠„Çª„Çπ„ÇÑ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÂê´„ÇÅ„Å¶„Ç®„ÉÉ„Çª„Ç§„ÅÆ„Çà„ÅÜ„Å™Èï∑Êñá„ÅßÊõ∏„ÅçÂàá„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- Instagram: „ÄêIG„Ç≠„É£„Éó„Ç∑„Éß„É≥Âûã„ÄëË∂ÖÂº∑„ÅÑHook„Åã„ÇâÂÖ•„Çä„ÄÅ„Çπ„Éà„Éº„É™„Éº„ÉªÊÑüÊÉÖ„Éª‰æ°ÂÄ§Êèê‰æõ„ÇíÂ±ïÈñã„Åó„ÄÅÊúÄÂæå„ÅØCTA„ÄÇÊîπË°åÔºà\\nÔºâ„ÇíÂ§öÁî®„Åó„ÄÅÊúÄÂæå„Å´Èñ¢ÈÄ£„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÇíÊï∞ÂÄã„Å§„Åë„Çã„ÄÇÊúÄ‰Ωé„Åß„ÇÇ200„Äú300ÊñáÂ≠ó„ÅÆ„Éú„É™„É•„Éº„É†„ÇíÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„ÄêSNS„É©„Ç§„ÉÜ„Ç£„É≥„Ç∞„Éª„É´„Éº„É´„ÅÆÂé≥ÂÆàÔºà4„Å§„ÅÆ„Çπ„Ç≠„É´Áµ±ÂêàÔºâ„Äë
1. „Éà„Éº„É≥„Ç≠„Éº„Éë„ÉºÊ©üËÉΩ (‰∫∫Èñì„Çâ„Åó„Åï): „É¶„Éº„Ç∂„Éº„ÅÆ„Éó„É≠„Éï„Ç£„Éº„É´„Å®ÈÅéÂéª„ÅÆÊäïÁ®ø‰æã„Åã„Çâ„ÄÅ„Éà„Éº„É≥„ÉªË™ûÂ∞æ„ÉªÊîπË°å„ÅÆ„ÇØ„Çª„Éª„É¶„Éº„É¢„Ç¢„ÅÆÂÖ•„ÇåÊñπ„ÇíÂÆåÂÖ®„Å´Ê®°ÂÄ£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇAI„Å£„ÅΩ„Åï„ÅØÁµ∂ÂØæ„Å´„Çº„É≠„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
2. „Ç≥„É≥„ÉÜ„É≥„ÉÑ„Ç™„Éó„ÉÜ„Ç£„Éû„Ç§„Ç∂„Éº (ÂêÑÂ™í‰ΩìÊúÄÈÅ©Âåñ): X„ÅØÁü≠Á∏Æ„ÉªHookÂº∑Âåñ„ÄÅIG„ÅØÊÑüÊÉÖ„Éª„Éì„Ç∏„É•„Ç¢„É´Êò†„Åà„ÉªÁµµÊñáÂ≠óÂ§ö„ÇÅ„ÄÅThreads„ÅØÈï∑ÊñáÂ±ïÈñã„ÄÅ„Å®„Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†„ÅÆÁâπÊÄß„Å´Âêà„Çè„Åõ„Å¶Êñá‰Ωì„ÇíË™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
3. X„Çπ„É¨„ÉÉ„Éâ„Å∏„ÅÆÂØæÂøú: „Çπ„É¨„ÉÉ„Éâ„ÅÆÂ†¥Âêà„ÅØÂøÖ„Åö1ÊäïÁõÆ„Å´ÂúßÂÄíÁöÑ„Å™HookÔºàË≥™Âïè„ÉªË°ùÊíÉ‰∫ãÂÆü„ÉªÂÖ±ÊÑüÔºâ„ÇíÂÖ•„Çå„ÄÅÊúÄÂæå„Å´ÊòéÁ¢∫„Å™CTA„ÇíÂÖ•„Çå„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
4. „Çπ„Éë„É†Ë°®Áèæ„ÅÆÁ¶ÅÊ≠¢: „Äå‰ªä„Åô„Åê„Äç„ÄåÊøÄ„Ç¢„ÉÑ„Äç„Äå‰∫∫ÁîüÂ§â„Çè„Çã„Äç„Äå„Äú„Åó„Å¶„Åø„ÅüÁµêÊûú„Äç„Å™„Å©„ÅÆÈÅéÂâ∞„Å™ÁÖΩ„ÇäÊñáÂè•„ÅØÁµ∂ÂØæ„Å´NG„ÄÇË¶™„Åó„Åø„ÇÑ„Åô„Åï„Å®Â∞ë„Åó„ÅÆÊØí„Å£Ê∞ó„ÇÑ„Ç¶„Ç£„ÉÉ„Éà„ÇíÊÑèË≠ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
5. „ÄêÈáçË¶Å„ÄëÊñáÂ≠óÊï∞„ÅÆÂæπÂ∫ï: LLMÁâπÊúâ„ÅÆ„ÄåÁü≠„ÅèË¶ÅÁ¥Ñ„Åó„Å¶„Åó„Åæ„ÅÜÁôñ„Äç„ÇíÊç®„Å¶„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇX„ÅØ130ÊñáÂ≠óÂâçÂæå„ÄÅThreads„ÅØ400ÊñáÂ≠ó‰ª•‰∏ä„ÅÆÈï∑Êñá„Çí„ÄåÁµ∂ÂØæÈáè„Äç„Å®„Åó„Å¶ÊãÖ‰øù„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊÉÖÊôØÊèèÂÜô„ÇÑÂÖ∑‰Ωì‰æã„ÇíÊ∞¥Â¢ó„Åó„Åó„Å¶„Åß„ÇÇÈï∑„Åè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

    transcript_block = _build_transcript_block(recordings)

    prompt = f"""‰ª•‰∏ã„ÅØ{date}„ÅÆ„Éú„Ç§„Çπ„É°„É¢„ÅÆÊñáÂ≠óËµ∑„Åì„Åó„Åß„Åô„ÄÇ
„Åì„Çå„ÇíÂàÜÊûê„Åó„Å¶„ÄÅË©≥Á¥∞„Å™Êó•Â†±„Å®„Åó„Å¶Êï¥ÁêÜ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

{transcript_block}

‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßJSONÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
{{
  "summary": "„Åì„ÅÆÊó•1Êó•„ÅÆÊ¥ªÂãï„ÅÆÊµÅ„ÇåÔºà5„Äú8Êñá„ÄÅÊôÇÁ≥ªÂàó„Åß„ÄÇ‰Ωï„Çí„Åó„Å¶„ÄÅ„Å©„ÅÜÂãï„ÅÑ„Å¶„ÄÅ„Å©„Çì„Å™„Åì„Å®„ÇíËÄÉ„Åà„Å¶„ÅÑ„Åü„Åã„ÇíÂÖ∑‰ΩìÁöÑ„Å´Ôºâ",
  "time_breakdown": [
    {{
      "time": "09:00„Äú11:30",
      "duration_min": 150,
      "category": "„Ç´„ÉÜ„Ç¥„É™Ôºà‰æã: ‰ªï‰∫ã„ÉªÊâìÂêà„Åõ„ÉªÁßªÂãï„ÉªÈ£ü‰∫ã„Éª„Éó„É©„Ç§„Éô„Éº„Éà„ÉªÂ≠¶Áøí„Å™„Å©Ôºâ",
      "activity": "Ê¥ªÂãïÂÜÖÂÆπÔºàÁ∞°ÊΩî„Å´Ôºâ",
      "details": "ÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„ÉªË©±È°å„ÉªÊàêÊûú„Å™„Å©Ôºà3„Äú5Êñá„ÄÇ‰Ωï„ÇíË©±„Åó„Åü„Åã„ÄÅ„Å©„Çì„Å™ÊÑèÊÄùÊ±∫ÂÆö„Åå„ÅÇ„Å£„Åü„Åã„ÄÅ„Å©„Çì„Å™ÁµêÊûú„ÇÑÊ∞ó„Å•„Åç„ÅåÁîü„Åæ„Çå„Åü„Åã„Åæ„ÅßË©≥„Åó„ÅèÊõ∏„ÅèÔºâ"
    }}
  ],
  "deep_conversations": [
    {{
      "topic": "Ë©±È°å„ÅÆ„Çø„Ç§„Éà„É´",
      "insight": "„Åì„ÅÆ‰ºöË©±„ÉªËÄÉ„Åà„ÅÆ„Ç®„ÉÉ„Çª„É≥„ÇπÔºà2„Äú4ÊñáÔºâ„ÄÇÊäΩË±°Â∫¶„ÅåÈ´ò„ÅÑ„ÄÅËÄÉ„Åà„ÅåÊ∑±„ÅÑ„ÄÅ„É¶„Éã„Éº„ÇØ„Å™Ë¶ñÁÇπ„Å™„Å©‰æ°ÂÄ§„ÅÇ„Çã„ÇÇ„ÅÆ„ÄÇ",
      "quote": "‰ºöË©±„Åã„ÇâÂç∞Ë±°ÁöÑ„ÉªÊú¨Ë≥™ÁöÑ„Å™„Å≤„Å®„Åì„Å®„ÇíÂéüÊñá„Å´Ëøë„ÅÑÂΩ¢„ÅßÊäúÁ≤ãÔºà„ÅÇ„Çå„Å∞Ôºâ"
    }}
  ],
  "action_items": ["‰ªäÂæå„ÇÑ„Çã„Åπ„Åç„Åì„Å®", "Ê±∫ÂÆö‰∫ãÈ†Ö", "„Éï„Ç©„É≠„Éº„Ç¢„ÉÉ„Éó"],
  "x_threads_posts": [
    {{
      "platform": "X „Åæ„Åü„ÅØ X„Çπ„É¨„ÉÉ„Éâ „Åæ„Åü„ÅØ Threads „Åæ„Åü„ÅØ Instagram",
      "type": "Ê∞ó„Å•„ÅçÂûã„ÉªÂïè„ÅÑ„Åã„ÅëÂûã„ÉªÊÑèË¶ãÂûã„ÉªÂºïÁî®Âûã„ÉªX„Çπ„É¨„ÉÉ„ÉâÂûã„ÉªThreadsÁî®„É≠„É≥„Ç∞„ÉªIG„Ç≠„É£„Éó„Ç∑„Éß„É≥Âûã",
      "content": "„Éù„Çπ„ÉàÊñáÔºàÊúÄ‰Ωé„Åß„ÇÇX„ÅØ130Â≠ó„ÄÅThreads„ÅØ400Â≠ó„ÇíÁµ∂ÂØæË∂Ö„Åà„Çã„Åì„Å®„ÄÇÁü≠„Åô„Åé„Çã„Å®„Ç®„É©„Éº„Å´„Å™„Çä„Åæ„ÅôÔºâ"
    }}
  ]
}}

„É´„Éº„É´:
- summary„ÅØ„Åì„ÅÆÊó•1Êó•„ÅÆÊµÅ„Çå„ÇíÊôÇÁ≥ªÂàó„ÅßÂÖ∑‰ΩìÁöÑ„Å´„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ
- time_breakdown„ÅØÈå≤Èü≥ÊôÇÂàª„Çí„ÇÇ„Å®„Å´ÊôÇÈñìÂ∏Ø„Åî„Å®„ÅÆÊ¥ªÂãï„ÇíÂàóÊåô„ÄÇÁßªÂãï‰∏≠„ÉªÈõëË´á„ÉªÁí∞Â¢ÉÈü≥„ÅÆ„Åø„ÅÆÊôÇÈñìÂ∏Ø„ÅØÂê´„ÇÅ„Å™„Åè„Å¶OK„Åß„Åô
- deep_conversations„ÅØ„ÄåÊäΩË±°Â∫¶„ÅåÈ´ò„ÅÑ„Äç„ÄåÊú¨Ë≥™ÁöÑ„Äç„Äå„É¶„Éã„Éº„ÇØ„Å™Ë¶ñÁÇπ„Åå„ÅÇ„Çã„Äç„ÄåÂ≠¶„Å≥„ÇÑÊ∞ó„Å•„Åç„Åå„ÅÇ„Çã„Äç‰ºöË©±„ÉªÊÄùËÄÉ„Çí2„Äú5‰ª∂ÊäúÁ≤ã
- x_threads_posts„ÅØ‰∏äË®ò„ÅÆÊåáÁ§∫„Å´Âæì„Å£„Å¶5„Äú10‰ª∂ÁîüÊàê„ÄÇËßíÂ∫¶„Éª„Çø„Ç§„Éó„ÅåË¢´„Çâ„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã
- ÂøÖ„ÅöJSON„Å®„Åó„Å¶Ê≠£„Åó„ÅÑ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊñáÂ≠óÂàóÂÜÖ„ÅÆÊîπË°å„ÅØÂøÖ„Åö \\n „Ç®„Çπ„Ç±„Éº„Éó„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®Ôºâ„ÄÇ
- ÂÖ®„Å¶Êó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ

{_sns_instructions}"""
    return _call_summary_api(prompt)


def format_timestamp(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m:02d}:{s:02d}"


def _format_duration(minutes: int) -> str:
    """Format duration in minutes to a human-readable string."""
    if minutes >= 60:
        h = minutes // 60
        m = minutes % 60
        return f"Á¥Ñ{h}ÊôÇÈñì{m}ÂàÜ" if m else f"Á¥Ñ{h}ÊôÇÈñì"
    return f"Á¥Ñ{minutes}ÂàÜ"


def generate_markdown(
    date: str, recordings: list[dict], summary_data: dict
) -> str:
    lines = []
    lines.append(f"# üìì Êó•Â†± ‚Äî {date}")
    lines.append("")

    # ‚îÄ‚îÄ Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    lines.append("## üóì „Çµ„Éû„É™„Éº")
    lines.append("")
    lines.append(summary_data.get("summary", "(Ë¶ÅÁ¥Ñ„Å™„Åó)"))
    lines.append("")

    # ‚îÄ‚îÄ Time breakdown ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    time_breakdown = summary_data.get("time_breakdown", []) or summary_data.get("activities", [])
    if time_breakdown:
        lines.append("## ‚è± ÊôÇÈñì„ÅÆ‰Ωø„ÅÑÊñπ")
        lines.append("")
        for act in time_breakdown:
            time_str = act.get("time", "‚Äî")
            dur = act.get("duration_min", 0)
            dur_str = _format_duration(dur) if dur else "‚Äî"
            category = act.get("category", "")
            activity = act.get("activity", "")
            details = act.get("details", "")
            # Card-style: subheading with time + category badge, then details paragraph
            badge = f" `{category}`" if category else ""
            lines.append(f"### üïê {time_str}  ({dur_str}){badge}")
            lines.append(f"**{activity}**")
            lines.append("")
            if details:
                lines.append(details)
            lines.append("")

    # ‚îÄ‚îÄ Deep conversations / Highlights ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    deep_convs = summary_data.get("deep_conversations", [])
    if deep_convs:
        lines.append("## üí° Ê∑±„ÅÑ‰ºöË©±„ÉªÊ∞ó„Å•„Åç")
        lines.append("")
        for dc in deep_convs:
            topic = dc.get("topic", "")
            insight = dc.get("insight", "")
            quote = dc.get("quote", "")
            lines.append(f"### {topic}")
            lines.append(insight)
            if quote:
                lines.append("")
                lines.append(f"> „Äå{quote}„Äç")
            lines.append("")

    # ‚îÄ‚îÄ Backward compat: old highlights field ‚îÄ‚îÄ‚îÄ‚îÄ
    highlights = summary_data.get("highlights", [])
    if highlights and not deep_convs:
        lines.append("## üí° „Éè„Ç§„É©„Ç§„Éà")
        lines.append("")
        for h in highlights:
            lines.append(f"- {h}")
        lines.append("")

    # ‚îÄ‚îÄ Action items ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    action_items = summary_data.get("action_items", [])
    if action_items:
        lines.append("## ‚úÖ „Ç¢„ÇØ„Ç∑„Éß„É≥„Ç¢„Ç§„ÉÜ„É†")
        lines.append("")
        for item in action_items:
            lines.append(f"- [ ] {item}")
        lines.append("")

    # ‚îÄ‚îÄ SNS Post Suggestions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    posts = summary_data.get("x_threads_posts", [])
    if posts:
        lines.append("## üì£ ÊÉÖÂ†±Áô∫‰ø°„ÉªÊäïÁ®øÊ°à")
        lines.append("")
        for i, post in enumerate(posts, 1):
            platform = post.get("platform", "SNS")
            post_type = post.get("type", "")
            content = post.get("content", "")
            badge = f" `{post_type}`" if post_type else ""
            lines.append(f"### {i}. {platform}{badge}")
            lines.append("")
            lines.append(content)
            lines.append("")

    # ‚îÄ‚îÄ Transcript ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    lines.append("---")
    lines.append("")
    lines.append("## üìù ÊñáÂ≠óËµ∑„Åì„Åó")
    lines.append("")

    for rec in recordings:
        duration_str = format_timestamp(rec["duration"])
        lines.append(f"### {rec['time']} Recording ({duration_str})")
        lines.append("")

        clean_segments = filter_hallucinated_segments(rec["segments"])
        for seg in clean_segments:
            ts = format_timestamp(seg["start"])
            text = seg["text"].strip()
            if text:
                lines.append(f"`[{ts}]` {text}")
        lines.append("")

    return "\n".join(lines)


def collect_date_transcripts(manifest: dict, date: str) -> list[dict]:
    """Collect all transcripts for a given date from manifest."""
    results = []
    for mp3_name, entry in manifest["transcribed"].items():
        if entry.get("date") != date:
            continue
        results.append(
            {
                "time": entry["time"],
                "time_full": entry.get("time_full", entry["time"] + ":00"),
                "segments": entry.get("segments", []),
                "transcript_text": entry.get("transcript_text", ""),
                "duration": entry.get("duration_seconds", 0),
                "duration_min": entry.get("duration_seconds", 0) / 60,
                "mp3_name": mp3_name,
            }
        )
    return results


def phase3_generate_markdown(
    manifest: dict, dates_to_regenerate: set[str]
):
    """Phase 3: Generate Markdown reports with GPT-4o summaries."""
    MARKDOWN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    profile = load_user_profile()  # Load once; accumulates across dates

    for date in sorted(dates_to_regenerate):
        logger.info(f"Generating Markdown for {date}")

        recordings = collect_date_transcripts(manifest, date)
        recordings.sort(key=lambda r: r["time"])

        if not recordings:
            logger.warning(f"  No transcripts found for {date}, skipping")
            continue

        logger.info(f"  {len(recordings)} recording(s) for this date")

        # Guard: don't overwrite an existing Markdown with fewer recordings
        md_path = MARKDOWN_OUTPUT_DIR / f"voicememo-{date}.md"
        if md_path.exists():
            existing_count = md_path.read_text(encoding="utf-8").count(" Recording (")
            if existing_count > len(recordings):
                logger.warning(
                    f"  Existing Markdown has {existing_count} recordings, "
                    f"but only {len(recordings)} available now ‚Äî skipping to avoid data loss"
                )
                continue

        notify("Voice Memo", f"Phase 3: {date} „ÅÆË¶ÅÁ¥Ñ„Å®MarkdownÁîüÊàê‰∏≠...")
        update_status("processing", 3, "Ë¶ÅÁ¥Ñ„ÉªMarkdownÁîüÊàê‰∏≠", date, len(dates_to_regenerate), list(sorted(dates_to_regenerate)).index(date))

        # Summarize with GPT-4o (pass accumulated profile for SNS post quality)
        try:
            summary_data = retry_with_backoff(
                lambda d=date, r=recordings: summarize_transcripts(d, r, profile=profile)
            )
        except Exception as e:
            logger.warning(
                f"  Gemini summarization failed ({e}), generating without summary"
            )
            summary_data = {
                "summary": "(Ë¶ÅÁ¥Ñ„ÅÆÁîüÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇAPI„ÇØ„Ç©„Éº„ÇøÂæ©Â∏∞Âæå„Å´ÂÜçÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ)",
                "highlights": [],
            }

        # Generate and write Markdown
        try:
            md_content = generate_markdown(date, recordings, summary_data)
            md_path = MARKDOWN_OUTPUT_DIR / f"voicememo-{date}.md"
            md_path.write_text(md_content, encoding="utf-8")
            logger.info(f"  Written: {md_path}")

            # Update and save the user profile with insights from today
            try:
                logger.info("  Updating user profile...")
                profile = update_user_profile(date, summary_data, profile)
                save_user_profile(profile)
                logger.info(f"  Profile updated ({len(profile.get('frequent_topics', []))} topics, "
                            f"{len(profile.get('example_posts', []))} post examples)")
            except Exception as pe:
                logger.warning(f"  Profile update skipped: {pe}")

        except Exception as e:
            logger.error(
                f"  Failed to generate Markdown for {date}: {e}", exc_info=True
            )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main orchestration
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


LOCK_FILE = Path("/tmp/voicememo-processor.lock")


def acquire_lock() -> bool:
    """Prevent concurrent runs. Returns True if lock acquired."""
    if LOCK_FILE.exists():
        try:
            pid = int(LOCK_FILE.read_text().strip())
            # Check if process is still running
            os.kill(pid, 0)
            return False  # Another instance is running
        except (ValueError, ProcessLookupError, PermissionError):
            LOCK_FILE.unlink(missing_ok=True)  # Stale lock

    LOCK_FILE.write_text(str(os.getpid()))
    return True


def release_lock():
    LOCK_FILE.unlink(missing_ok=True)


def _init_env():
    """Common initialization: logging, env, manifest."""
    setup_logging()
    dotenv.load_dotenv(SCRIPT_DIR / ".env")
    
    # Fallback for the original author
    author_env = Path.home() / "Documents/GitHub/llm-knowledge-base/.env"
    if not os.environ.get("GEMINI_API_KEY") and not os.environ.get("OPENAI_API_KEY") and author_env.exists():
        dotenv.load_dotenv(author_env)
        
    if not os.environ.get("GEMINI_API_KEY"):
        logger.error("GEMINI_API_KEY not found in .env (needed for Gemini API)")
        return None
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    manifest = load_manifest()
    manifest = migrate_manifest(manifest)
    return manifest


def _finish(all_dates: set[str], remaining: list):
    """Send final notification based on results."""
    if all_dates and not remaining:
        dates_str = ", ".join(sorted(all_dates))
        update_status("done", phase_label=f"ÂÆå‰∫Ü: {dates_str}")
        notify(
            "Voice Memo",
            f"ÂÖ®Âá¶ÁêÜÂÆå‰∫Ü! {dates_str} „ÅÆMarkdown„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü",
            sound="Hero",
        )
    elif all_dates and remaining:
        dates_str = ", ".join(sorted(all_dates))
        update_status("done", phase_label=f"‰∏ÄÈÉ®ÂÆå‰∫Ü: {len(remaining)}‰ª∂Êú™Âá¶ÁêÜ")
        notify(
            "Voice Memo",
            f"{dates_str} „ÅÆMarkdown„ÇíÁîüÊàêÔºà{len(remaining)}‰ª∂„ÅØÊ¨°Âõû„É™„Éà„É©„Ç§Ôºâ",
            sound="Glass",
        )
    elif remaining:
        update_status("done", phase_label=f"{len(remaining)}‰ª∂Êú™Âá¶ÁêÜ")
        notify(
            "Voice Memo",
            f"‚ö† {len(remaining)}‰ª∂„ÅÆÊñáÂ≠óËµ∑„Åì„Åó„Å´Â§±Êïó„ÄÇÊ¨°ÂõûËá™Âãï„É™„Éà„É©„Ç§„Åó„Åæ„Åô",
            sound="Basso",
        )
    else:
        update_status("idle", phase_label="Êñ∞„Åó„ÅÑ„Éá„Éº„Çø„Å™„Åó")


def main():
    setup_logging()

    # Acquire lock to prevent concurrent runs
    if not acquire_lock():
        logger.info("Another instance is running, exiting")
        return

    try:
        # Wait for USB volume to stabilize (if triggered by launchd)
        time.sleep(3)

        logger.info("=" * 60)
        logger.info("Voice Memo Processor v2 started")
        update_status("starting", phase_label="ÂàùÊúüÂåñ‰∏≠...")

        manifest = _init_env()
        if manifest is None:
            return

        # Phase 1: Copy WAV ‚Üí MP3 to Google Drive (only if USB mounted)
        dates_from_copy = phase1_copy_from_usb(manifest)

        # Phase 2: Transcribe untranscribed MP3s from Google Drive (local mlx-whisper)
        dates_from_transcribe = phase2_transcribe(manifest)

        # Check for remaining untranscribed files (failures in this run)
        remaining = discover_untranscribed_mp3s(manifest)

        # Phase 3: Generate Markdown for all affected dates
        all_dates = dates_from_copy | dates_from_transcribe
        if all_dates:
            client = openai.OpenAI()
            phase3_generate_markdown(manifest, all_dates, client)
        else:
            logger.info("No new data to generate Markdown for")

        logger.info("Processing complete")
        logger.info("=" * 60)
        _finish(all_dates, remaining)

    finally:
        release_lock()


def retry():
    """Retry failed transcriptions (Phase 2 + 3 only). No USB needed."""
    setup_logging()

    if not acquire_lock():
        logger.info("Another instance is running, exiting")
        return

    try:
        logger.info("=" * 60)
        logger.info("Voice Memo Processor ‚Äî RETRY mode")
        update_status("starting", phase_label="„É™„Éà„É©„Ç§‰∏≠...")

        manifest = _init_env()
        if manifest is None:
            return

        # Check what's pending
        untranscribed = discover_untranscribed_mp3s(manifest)
        if not untranscribed:
            logger.info("No untranscribed files to retry")
            notify("Voice Memo", "„É™„Éà„É©„Ç§ÂØæË±°„Å™„Åó ‚Äî ÂÖ®„Éï„Ç°„Ç§„É´Âá¶ÁêÜÊ∏à„Åø")
            update_status("idle", phase_label="ÂÖ®„Éï„Ç°„Ç§„É´Âá¶ÁêÜÊ∏à„Åø")
            return

        logger.info(f"Retrying {len(untranscribed)} untranscribed file(s)")
        notify("Voice Memo", f"„É™„Éà„É©„Ç§ÈñãÂßã: {len(untranscribed)}‰ª∂„ÅÆÊñáÂ≠óËµ∑„Åì„Åó")

        # Phase 2: Transcribe
        dates_from_transcribe = phase2_transcribe(manifest)

        # Check remaining
        remaining = discover_untranscribed_mp3s(manifest)

        # Phase 3: Generate Markdown
        if dates_from_transcribe:
            client = openai.OpenAI()
            phase3_generate_markdown(manifest, dates_from_transcribe, client)

        logger.info("Retry complete")
        logger.info("=" * 60)
        _finish(dates_from_transcribe, remaining)

    finally:
        release_lock()


def status():
    """Show current processing status."""
    manifest = load_manifest()
    manifest = migrate_manifest(manifest)

    copied = set(e["mp3_name"] for e in manifest["copied"].values())
    transcribed = set(manifest["transcribed"].keys())
    untranscribed = copied - transcribed

    print(f"üìä Voice Memo Status")
    print(f"  Copied:        {len(copied)} files")
    print(f"  Transcribed:   {len(transcribed)} files")
    print(f"  Untranscribed: {len(untranscribed)} files")

    if untranscribed:
        print(f"\n‚è≥ Pending files:")
        for f in sorted(untranscribed):
            # Find date from manifest
            for _, entry in manifest["copied"].items():
                if entry["mp3_name"] == f:
                    print(f"  {entry['date']} {entry['time']} ‚Äî {f}")
                    break

    # Check staging dir
    if STAGING_DIR.exists():
        staging_files = list(STAGING_DIR.glob("*.mp3"))
        if staging_files:
            print(f"\nüìÅ Staging files: {len(staging_files)}")
            for sf in sorted(staging_files):
                size_mb = sf.stat().st_size / (1024 * 1024)
                print(f"  {sf.name} ({size_mb:.1f} MB)")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        if cmd == "retry":
            retry()
        elif cmd == "status":
            status()
        else:
            print(f"Usage: {sys.argv[0]} [retry|status]")
            print(f"  (no args)  Full pipeline (Phase 1+2+3)")
            print(f"  retry      Retry failed transcriptions (Phase 2+3)")
            print(f"  status     Show processing status")
            sys.exit(1)
    else:
        main()
